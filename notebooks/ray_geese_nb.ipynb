{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib - Extra Application Example - Taxi-v3\n",
    "\n",
    "© 2019-2021, Anyscale. All Rights Reserved\n",
    "\n",
    "This example uses [RLlib](https://ray.readthedocs.io/en/latest/rllib.html) to train a policy with the `Taxi-v3` environment ([gym.openai.com/envs/Taxi-v3/](https://gym.openai.com/envs/Taxi-v3/)). The goal is to pick up passengers as fast as possible, negotiating the available paths. This is one of OpenAI Gym's [\"toy text\"](https://gym.openai.com/envs/#toy_text) problems.\n",
    "\n",
    "For more background about this problem, see:\n",
    "\n",
    "* [\"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\"](https://arxiv.org/abs/cs/9905014), [Thomas G. Dietteric](https://twitter.com/tdietterich)\n",
    "* [\"Reinforcement Learning: let’s teach a taxi-cab how to drive\"](https://towardsdatascience.com/reinforcement-learning-lets-teach-a-taxi-cab-how-to-drive-4fd1a0d00529), [Valentina Alto](https://twitter.com/AltoValentina)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rhys/Projects/RLxEvolution/RLxEvolution/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "Loading environment football failed: No module named 'gfootball'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import ray\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "import ray.rllib.agents.sac as sac\n",
    "\n",
    "\n",
    "from EnvWrapperRay import HungryGeeseKaggle\n",
    "from ray_utils import CustomModel, CustomConvModel\n",
    "from ray.rllib.models import ModelCatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ray.shutdown()\n",
    "except:\n",
    "    print('not running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-08 14:34:17,097\tINFO services.py:1267 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "info = ray.init(ignore_reinit_error=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard URL: http://127.0.0.1:8266\n"
     ]
    }
   ],
   "source": [
    "print(\"Dashboard URL: http://{}\".format(info[\"webui_url\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the checkpoint location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_root = \"tmp/ppo-lstm/geese\"\n",
    "shutil.rmtree(checkpoint_root, ignore_errors=True, onerror=None)   # clean up old runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll train an RLlib policy\n",
    "\n",
    "By default, training runs for `10` iterations. Increase the `N_ITER` setting if you want to see the resulting rewards improve.\n",
    "Also note that *checkpoints* get saved after each iteration into the `/tmp/ppo/taxi` directory.\n",
    "\n",
    "> **Note:** If you prefer to use a different directory root than `/tmp`, change it in the next cell **and** in the `rllib rollout` command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"my_model\", CustomConvModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m WARNING:tensorflow:From /home/rhys/Projects/RLxEvolution/RLxEvolution/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m WARNING:tensorflow:From /home/rhys/Projects/RLxEvolution/RLxEvolution/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m Loading environment football failed: No module named 'gfootball'\n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m Loading environment football failed: No module named 'gfootball'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m WARNING:tensorflow:From /home/rhys/Projects/RLxEvolution/RLxEvolution/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1659: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m WARNING:tensorflow:From /home/rhys/Projects/RLxEvolution/RLxEvolution/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1659: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m Model: \"model_1\"\n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m ==================================================================================================\n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m seq_in (InputLayer)             [(None,)]            0                                            \n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m tf_op_layer_default_policy/Sequ [()]                 0           seq_in[0][0]                     \n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m tf_op_layer_default_policy/Sequ [()]                 0           tf_op_layer_default_policy/Sequen\n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m tf_op_layer_default_policy/Sequ [(None, 1)]          0           seq_in[0][0]                     \n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m tf_op_layer_default_policy/Sequ [(None,)]            0           tf_op_layer_default_policy/Sequen\n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m tf_op_layer_default_policy/Sequ [(None, 1)]          0           tf_op_layer_default_policy/Sequen\n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m inputs (InputLayer)             [(None, None, 7)]    0                                            \n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m h (InputLayer)                  [(None, 256)]        0                                            \n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m c (InputLayer)                  [(None, 256)]        0                                            \n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m tf_op_layer_default_policy/Sequ [(None, None)]       0           tf_op_layer_default_policy/Sequen\n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m                                                                  tf_op_layer_default_policy/Sequen\n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m lstm (LSTM)                     [(None, None, 256),  270336      inputs[0][0]                     \n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m                                                                  h[0][0]                          \n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m                                                                  c[0][0]                          \n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m logits (Dense)                  (None, None, 4)      1028        lstm[0][0]                       \n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m values (Dense)                  (None, None, 1)      257         lstm[0][0]                       \n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m ==================================================================================================\n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m Total params: 271,621\n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m Trainable params: 271,621\n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m Non-trainable params: 0\n",
      "\u001b[2m\u001b[36m(pid=38188)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m Model: \"model_1\"\n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m ==================================================================================================\n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m seq_in (InputLayer)             [(None,)]            0                                            \n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m tf_op_layer_default_policy/Sequ [()]                 0           seq_in[0][0]                     \n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m tf_op_layer_default_policy/Sequ [()]                 0           tf_op_layer_default_policy/Sequen\n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m tf_op_layer_default_policy/Sequ [(None, 1)]          0           seq_in[0][0]                     \n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m tf_op_layer_default_policy/Sequ [(None,)]            0           tf_op_layer_default_policy/Sequen\n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m tf_op_layer_default_policy/Sequ [(None, 1)]          0           tf_op_layer_default_policy/Sequen\n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m inputs (InputLayer)             [(None, None, 7)]    0                                            \n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m h (InputLayer)                  [(None, 256)]        0                                            \n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m c (InputLayer)                  [(None, 256)]        0                                            \n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m tf_op_layer_default_policy/Sequ [(None, None)]       0           tf_op_layer_default_policy/Sequen\n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m                                                                  tf_op_layer_default_policy/Sequen\n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m lstm (LSTM)                     [(None, None, 256),  270336      inputs[0][0]                     \n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m                                                                  h[0][0]                          \n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m                                                                  c[0][0]                          \n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m logits (Dense)                  (None, None, 4)      1028        lstm[0][0]                       \n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m values (Dense)                  (None, None, 1)      257         lstm[0][0]                       \n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m ==================================================================================================\n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m Total params: 271,621\n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m Trainable params: 271,621\n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m Non-trainable params: 0\n",
      "\u001b[2m\u001b[36m(pid=38194)\u001b[0m __________________________________________________________________________________________________\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "seq_in (InputLayer)             [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_default_policy/Sequ [()]                 0           seq_in[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_default_policy/Sequ [()]                 0           tf_op_layer_default_policy/Sequen\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_default_policy/Sequ [(None, 1)]          0           seq_in[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_default_policy/Sequ [(None,)]            0           tf_op_layer_default_policy/Sequen\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_default_policy/Sequ [(None, 1)]          0           tf_op_layer_default_policy/Sequen\n",
      "__________________________________________________________________________________________________\n",
      "inputs (InputLayer)             [(None, None, 7)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "h (InputLayer)                  [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "c (InputLayer)                  [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_default_policy/Sequ [(None, None)]       0           tf_op_layer_default_policy/Sequen\n",
      "                                                                 tf_op_layer_default_policy/Sequen\n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, None, 256),  270336      inputs[0][0]                     \n",
      "                                                                 h[0][0]                          \n",
      "                                                                 c[0][0]                          \n",
      "__________________________________________________________________________________________________\n",
      "logits (Dense)                  (None, None, 4)      1028        lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "values (Dense)                  (None, None, 1)      257         lstm[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 271,621\n",
      "Trainable params: 271,621\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-08 14:41:43,599\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "N_ITER = 1000\n",
    "\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config[\"log_level\"] = \"WARN\"\n",
    "config['model'] = {\n",
    "#           \"custom_model\": \"my_model\"\n",
    "        \"conv_filters\": [[32, 16, 2], [3,4,8]],\n",
    "        \"use_lstm\": True,\n",
    "        \"lstm_use_prev_action\": True,\n",
    "        #\"fcnet_hiddens\": [256,256],\n",
    "        #\"fcnet_activation\": \"relu\",\n",
    "      }\n",
    "#config['framework'] = 'tfe'\n",
    "#config[\"eager_tracing\"] =  True\n",
    "#config[\"num_gpus\"] = 0\n",
    "#config['conv_filters'] = [[32,16,1]]\n",
    "\n",
    "#config['Q_model'] = {\n",
    "#           \"custom_model\": \"my_model\"\n",
    "#        }\n",
    "config[\"no_done_at_end\"] = True\n",
    "config[\"normalize_actions\"] =  False\n",
    "config['vf_clip_param'] = 100\n",
    "\n",
    "agent = ppo.PPOTrainer(config, env=HungryGeeseKaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1: Min/Mean/Max reward: -187.9036/ 80.3153/627.9761, len mean:   5.1373. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000001/checkpoint-1\n",
      "  2: Min/Mean/Max reward: -349.3524/ 70.1586/634.2083, len mean:   5.0083. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000002/checkpoint-2\n",
      "  3: Min/Mean/Max reward: -240.6193/ 89.2747/575.2131, len mean:   5.2368. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000003/checkpoint-3\n",
      "  4: Min/Mean/Max reward: -256.5463/106.2915/732.7177, len mean:   5.0420. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000004/checkpoint-4\n",
      "  5: Min/Mean/Max reward: -310.5533/ 91.9799/812.8545, len mean:   5.1416. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000005/checkpoint-5\n",
      "  6: Min/Mean/Max reward: -413.9616/ 92.3674/569.5937, len mean:   5.2026. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000006/checkpoint-6\n",
      "  7: Min/Mean/Max reward: -191.6667/102.8590/736.8381, len mean:   5.0983. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000007/checkpoint-7\n",
      "  8: Min/Mean/Max reward: -281.0983/ 71.3286/475.3322, len mean:   4.8240. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000008/checkpoint-8\n",
      "  9: Min/Mean/Max reward: -217.9779/ 83.2098/857.7047, len mean:   5.1416. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000009/checkpoint-9\n",
      " 10: Min/Mean/Max reward: -212.3436/ 68.0482/503.8976, len mean:   4.7920. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000010/checkpoint-10\n",
      " 11: Min/Mean/Max reward: -337.1966/ 72.3002/535.9886, len mean:   4.9671. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000011/checkpoint-11\n",
      " 12: Min/Mean/Max reward: -609.4317/ 81.4884/605.3707, len mean:   4.9218. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000012/checkpoint-12\n",
      " 13: Min/Mean/Max reward: -268.8215/ 74.0050/457.4459, len mean:   5.0209. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000013/checkpoint-13\n",
      " 14: Min/Mean/Max reward: -216.1166/ 79.6397/589.8333, len mean:   5.2445. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000014/checkpoint-14\n",
      " 15: Min/Mean/Max reward: -425.0865/ 73.7920/502.8427, len mean:   5.0809. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000015/checkpoint-15\n",
      " 16: Min/Mean/Max reward: -215.0000/ 83.9198/560.3621, len mean:   5.1229. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000016/checkpoint-16\n",
      " 17: Min/Mean/Max reward: -418.6667/ 66.9490/572.2232, len mean:   5.2876. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000017/checkpoint-17\n",
      " 18: Min/Mean/Max reward: -322.2871/ 75.2735/533.4908, len mean:   4.9547. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000018/checkpoint-18\n",
      " 19: Min/Mean/Max reward: -229.0983/ 76.5998/467.1981, len mean:   5.1342. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000019/checkpoint-19\n",
      " 20: Min/Mean/Max reward: -346.4316/ 71.4278/550.9225, len mean:   4.9508. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000020/checkpoint-20\n",
      " 21: Min/Mean/Max reward: -649.7397/ 74.3475/621.6276, len mean:   5.1277. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000021/checkpoint-21\n",
      " 22: Min/Mean/Max reward: -333.0592/ 86.6731/579.0510, len mean:   5.0127. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000022/checkpoint-22\n",
      " 23: Min/Mean/Max reward: -304.7368/ 67.0655/579.8324, len mean:   5.2078. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000023/checkpoint-23\n",
      " 24: Min/Mean/Max reward: -458.2871/ 69.7001/775.8970, len mean:   5.1271. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000024/checkpoint-24\n",
      " 25: Min/Mean/Max reward: -512.1084/ 65.5463/536.5581, len mean:   5.0805. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000025/checkpoint-25\n",
      " 26: Min/Mean/Max reward: -255.0576/ 66.6791/437.8551, len mean:   4.9504. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000026/checkpoint-26\n",
      " 27: Min/Mean/Max reward: -267.7397/ 84.1385/636.4409, len mean:   5.2130. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000027/checkpoint-27\n",
      " 28: Min/Mean/Max reward: -182.9223/ 69.4648/611.9897, len mean:   5.3053. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000028/checkpoint-28\n",
      " 29: Min/Mean/Max reward: -258.4585/ 86.4411/560.1645, len mean:   5.1674. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000029/checkpoint-29\n",
      " 30: Min/Mean/Max reward: -607.9616/ 90.2886/837.9401, len mean:   5.2358. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000030/checkpoint-30\n",
      " 31: Min/Mean/Max reward: -282.4583/ 76.8656/626.3634, len mean:   5.2500. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000031/checkpoint-31\n",
      " 32: Min/Mean/Max reward: -225.6724/110.8594/825.1752, len mean:   5.6028. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000032/checkpoint-32\n",
      " 33: Min/Mean/Max reward: -336.5936/ 77.6534/604.1322, len mean:   5.3467. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000033/checkpoint-33\n",
      " 34: Min/Mean/Max reward: -390.6054/ 89.6363/743.5928, len mean:   5.2237. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000034/checkpoint-34\n",
      " 35: Min/Mean/Max reward: -279.5566/ 89.4276/688.1369, len mean:   5.3901. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000035/checkpoint-35\n",
      " 36: Min/Mean/Max reward: -366.0000/ 88.2363/530.1287, len mean:   5.3009. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000036/checkpoint-36\n",
      " 37: Min/Mean/Max reward: -318.0327/ 92.1335/602.6666, len mean:   5.5438. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000037/checkpoint-37\n",
      " 38: Min/Mean/Max reward: -500.4806/ 85.1577/691.4258, len mean:   5.7014. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000038/checkpoint-38\n",
      " 39: Min/Mean/Max reward: -323.7650/ 81.3786/796.8693, len mean:   5.7062. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000039/checkpoint-39\n",
      " 40: Min/Mean/Max reward: -631.2949/ 98.0512/527.6060, len mean:   5.5300. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000040/checkpoint-40\n",
      " 41: Min/Mean/Max reward: -308.2453/ 92.2855/536.0831, len mean:   5.7321. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000041/checkpoint-41\n",
      " 42: Min/Mean/Max reward: -228.7693/116.2205/884.5198, len mean:   6.0402. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000042/checkpoint-42\n",
      " 43: Min/Mean/Max reward: -249.3183/ 89.9857/491.7800, len mean:   5.7548. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000043/checkpoint-43\n",
      " 44: Min/Mean/Max reward: -321.5967/ 94.5059/525.3600, len mean:   6.0402. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000044/checkpoint-44\n",
      " 45: Min/Mean/Max reward: -283.0615/ 97.9236/611.6564, len mean:   6.1649. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000045/checkpoint-45\n",
      " 46: Min/Mean/Max reward: -166.4231/103.6435/747.7713, len mean:   5.9307. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000046/checkpoint-46\n",
      " 47: Min/Mean/Max reward: -196.2590/115.4007/704.1059, len mean:   6.5326. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000047/checkpoint-47\n",
      " 48: Min/Mean/Max reward: -250.4567/123.8442/797.9140, len mean:   6.6740. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000048/checkpoint-48\n",
      " 49: Min/Mean/Max reward: -173.8139/103.4872/600.6053, len mean:   6.2737. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000049/checkpoint-49\n",
      " 50: Min/Mean/Max reward: -298.1652/110.4167/710.0690, len mean:   6.1487. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000050/checkpoint-50\n",
      " 51: Min/Mean/Max reward: -366.0845/115.1176/788.9090, len mean:   6.5489. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000051/checkpoint-51\n",
      " 52: Min/Mean/Max reward: -287.3000/114.8473/631.5722, len mean:   6.7684. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000052/checkpoint-52\n",
      " 53: Min/Mean/Max reward: -642.1902/138.8737/762.0925, len mean:   6.3158. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000053/checkpoint-53\n",
      " 54: Min/Mean/Max reward: -550.5041/ 91.8034/551.2339, len mean:   6.6910. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000054/checkpoint-54\n",
      " 55: Min/Mean/Max reward: -168.9616/153.2437/645.0287, len mean:   7.4907. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000055/checkpoint-55\n",
      " 56: Min/Mean/Max reward: -180.1039/129.2300/932.4600, len mean:   7.2169. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000056/checkpoint-56\n",
      " 57: Min/Mean/Max reward: -183.9318/142.9572/817.6921, len mean:   6.8079. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000057/checkpoint-57\n",
      " 58: Min/Mean/Max reward: -250.1826/159.4387/834.4817, len mean:   7.7974. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000058/checkpoint-58\n",
      " 59: Min/Mean/Max reward: -493.4791/114.0207/721.2118, len mean:   7.7548. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000059/checkpoint-59\n",
      " 60: Min/Mean/Max reward: -272.9526/158.7465/919.7364, len mean:   7.0234. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000060/checkpoint-60\n",
      " 61: Min/Mean/Max reward: -660.3854/136.7332/741.6205, len mean:   7.7161. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000061/checkpoint-61\n",
      " 62: Min/Mean/Max reward: -344.5566/161.4878/708.3453, len mean:   8.5070. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000062/checkpoint-62\n",
      " 63: Min/Mean/Max reward: -327.2575/161.7647/849.7094, len mean:   7.9801. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000063/checkpoint-63\n",
      " 64: Min/Mean/Max reward: -263.8796/175.8061/810.0429, len mean:   8.1633. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000064/checkpoint-64\n",
      " 65: Min/Mean/Max reward: -776.1272/167.5130/798.6902, len mean:   8.2414. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000065/checkpoint-65\n",
      " 66: Min/Mean/Max reward: -222.7972/157.5707/902.5252, len mean:   8.7591. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000066/checkpoint-66\n",
      " 67: Min/Mean/Max reward: -204.7989/168.9967/788.6250, len mean:   9.0000. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000067/checkpoint-67\n",
      " 68: Min/Mean/Max reward: -671.6446/211.2033/792.6950, len mean:   9.0376. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000068/checkpoint-68\n",
      " 69: Min/Mean/Max reward: -295.0722/168.2703/917.7529, len mean:   9.4331. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000069/checkpoint-69\n",
      " 70: Min/Mean/Max reward: -176.1249/212.6516/712.7503, len mean:  10.5000. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000070/checkpoint-70\n",
      " 71: Min/Mean/Max reward: -232.0127/223.4115/1219.4746, len mean:  10.2650. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000071/checkpoint-71\n",
      " 72: Min/Mean/Max reward: -357.6025/184.2553/842.0511, len mean:   9.7886. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000072/checkpoint-72\n",
      " 73: Min/Mean/Max reward: -129.8543/194.0513/946.8044, len mean:  10.0000. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000073/checkpoint-73\n",
      " 74: Min/Mean/Max reward: -169.8412/214.6883/1120.7090, len mean:  10.3362. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000074/checkpoint-74\n",
      " 75: Min/Mean/Max reward: -231.7495/236.4599/906.0269, len mean:  11.1028. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000075/checkpoint-75\n",
      " 76: Min/Mean/Max reward: -101.7058/228.3887/884.0886, len mean:  11.2430. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000076/checkpoint-76\n",
      " 77: Min/Mean/Max reward: -262.5103/253.0206/794.3660, len mean:  11.1852. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000077/checkpoint-77\n",
      " 78: Min/Mean/Max reward: -103.3333/277.1562/955.3090, len mean:  12.3000. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000078/checkpoint-78\n",
      " 79: Min/Mean/Max reward: -136.4257/314.4211/1070.9795, len mean:  12.2300. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000079/checkpoint-79\n",
      " 80: Min/Mean/Max reward: -151.2624/289.0734/977.7207, len mean:  12.3200. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000080/checkpoint-80\n",
      " 81: Min/Mean/Max reward: -226.9373/299.0027/976.0442, len mean:  13.8600. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000081/checkpoint-81\n",
      " 82: Min/Mean/Max reward: -226.9373/297.9920/1037.9557, len mean:  13.4400. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000082/checkpoint-82\n",
      " 83: Min/Mean/Max reward: -83.3229/373.7525/1345.8560, len mean:  15.6100. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000083/checkpoint-83\n",
      " 84: Min/Mean/Max reward: -109.4376/404.3456/1146.0410, len mean:  17.3300. Checkpoint saved to tmp/ppo-lstm/geese/checkpoint_000084/checkpoint-84\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "episode_data = []\n",
    "episode_json = []\n",
    "\n",
    "for n in range(N_ITER):\n",
    "    result = agent.train()\n",
    "    results.append(result)\n",
    "    \n",
    "    episode = {'n': n, \n",
    "               'episode_reward_min': result['episode_reward_min'], \n",
    "               'episode_reward_mean': result['episode_reward_mean'], \n",
    "               'episode_reward_max': result['episode_reward_max'],  \n",
    "               'episode_len_mean': result['episode_len_mean']\n",
    "              }\n",
    "    \n",
    "    episode_data.append(episode)\n",
    "    episode_json.append(json.dumps(episode))\n",
    "    file_name = agent.save(checkpoint_root)\n",
    "    \n",
    "    print(f'{n+1:3d}: Min/Mean/Max reward: {result[\"episode_reward_min\"]:8.4f}/{result[\"episode_reward_mean\"]:8.4f}/{result[\"episode_reward_max\"]:8.4f}, len mean: {result[\"episode_len_mean\"]:8.4f}. Checkpoint saved to {file_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gp1LgeCJjGLk"
   },
   "source": [
    "Do the episode rewards increase after multiple iterations?\n",
    "\n",
    "Also, print out the policy and model to see the results of training in detail…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "policy = agent.get_policy()\n",
    "model = policy.model\n",
    "\n",
    "print('variables')\n",
    "pprint.pprint(model.variables())\n",
    "pprint.pprint(model.value_function())\n",
    "policy.export_model('./submission_ray/model')\n",
    "print(model.base_model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollout\n",
    "\n",
    "Next we'll use the [`rollout` script](https://ray.readthedocs.io/en/latest/rllib-training.html#evaluating-trained-policies) to evaluate the trained policy.\n",
    "\n",
    "The output from the following command visualizes the \"taxi\" agent operating within its simulation: picking up a passenger, driving, turning, dropping off a passenger (\"put-down\"), and so on. \n",
    "\n",
    "A 2-D map of the *observation space* is visualized as text, which needs some decoding instructions:\n",
    "\n",
    "  * `R` -- R(ed) location in the Northwest corner\n",
    "  * `G` -- G(reen) location in the Northeast corner\n",
    "  * `Y` -- Y(ellow) location in the Southwest corner\n",
    "  * `B` -- B(lue) location in the Southeast corner\n",
    "  * `:` -- cells where the taxi can drive\n",
    "  * `|` -- obstructions (\"walls\") which the taxi must avoid\n",
    "  * blue letter represents the current passenger’s location for pick-up\n",
    "  * purple letter represents the drop-off location\n",
    "  * yellow rectangle is the current location of our taxi/agent\n",
    "\n",
    "That allows for a total of 500 states, and these known states are numbered between 0 and 499.\n",
    "\n",
    "The *action space* for the taxi/agent is defined as:\n",
    "\n",
    "  * move the taxi one square North\n",
    "  * move the taxi one square South\n",
    "  * move the taxi one square East\n",
    "  * move the taxi one square West\n",
    "  * pick-up the passenger\n",
    "  * put-down the passenger\n",
    "\n",
    "The *rewards* are structured as −1 for each action plus:\n",
    "\n",
    " * +20 points when the taxi performs a correct drop-off for the passenger\n",
    " * -10 points when the taxi attempts illegal pick-up/drop-off actions\n",
    "\n",
    "Admittedly it'd be better if these state visualizations showed the *reward* along with observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = 'tmp/ppo/geese/checkpoint_000001/checkpoint-1'\n",
    "agent = ppo.PPOTrainer(config, env=HungryGeeseKaggle)\n",
    "agent.restore(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = agent.get_policy()\n",
    "model = policy.model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.compute_single_action()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!rllib rollout \\\n",
    "    tmp/ppo/taxi/checkpoint_000010/checkpoint-10 \\\n",
    "    --config \"{\\\"env\\\": \\\"Taxi-v3\\\"}\" \\\n",
    "    --run PPO \\\n",
    "    --steps 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()  # \"Undo ray.init()\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise (\"Homework\")\n",
    "\n",
    "In addition to _Taxi_, there are other so-called [\"toy text\"](https://gym.openai.com/envs/#toy_text) problems you can try."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of rllib_ppo_dqn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "RLxEvolution",
   "language": "python",
   "name": "rlxevolution"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
