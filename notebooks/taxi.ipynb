{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib - Extra Application Example - Taxi-v3\n",
    "\n",
    "© 2019-2021, Anyscale. All Rights Reserved\n",
    "\n",
    "This example uses [RLlib](https://ray.readthedocs.io/en/latest/rllib.html) to train a policy with the `Taxi-v3` environment ([gym.openai.com/envs/Taxi-v3/](https://gym.openai.com/envs/Taxi-v3/)). The goal is to pick up passengers as fast as possible, negotiating the available paths. This is one of OpenAI Gym's [\"toy text\"](https://gym.openai.com/envs/#toy_text) problems.\n",
    "\n",
    "For more background about this problem, see:\n",
    "\n",
    "* [\"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\"](https://arxiv.org/abs/cs/9905014), [Thomas G. Dietteric](https://twitter.com/tdietterich)\n",
    "* [\"Reinforcement Learning: let’s teach a taxi-cab how to drive\"](https://towardsdatascience.com/reinforcement-learning-lets-teach-a-taxi-cab-how-to-drive-4fd1a0d00529), [Valentina Alto](https://twitter.com/AltoValentina)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rhys/Projects/RLxEvolution/RLxEvolution/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import ray\n",
    "import ray.rllib.agents.ppo as ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-03 16:17:40,870\tINFO services.py:1267 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "info = ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard URL: http://127.0.0.1:8265\n"
     ]
    }
   ],
   "source": [
    "print(\"Dashboard URL: http://{}\".format(info[\"webui_url\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the checkpoint location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_root = \"tmp/ppo/taxi\"\n",
    "shutil.rmtree(checkpoint_root, ignore_errors=True, onerror=None)   # clean up old runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll train an RLlib policy with the `Taxi-v3` environment.\n",
    "\n",
    "By default, training runs for `10` iterations. Increase the `N_ITER` setting if you want to see the resulting rewards improve.\n",
    "Also note that *checkpoints* get saved after each iteration into the `/tmp/ppo/taxi` directory.\n",
    "\n",
    "> **Note:** If you prefer to use a different directory root than `/tmp`, change it in the next cell **and** in the `rllib rollout` command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-03 16:17:43,079\tINFO trainer.py:669 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2021-06-03 16:17:43,081\tINFO trainer.py:694 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=164530)\u001b[0m WARNING:tensorflow:From /home/rhys/Projects/RLxEvolution/RLxEvolution/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=164530)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=164530)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=164525)\u001b[0m WARNING:tensorflow:From /home/rhys/Projects/RLxEvolution/RLxEvolution/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=164525)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=164525)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=164530)\u001b[0m WARNING:tensorflow:From /home/rhys/Projects/RLxEvolution/RLxEvolution/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1659: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=164530)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=164530)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=164525)\u001b[0m WARNING:tensorflow:From /home/rhys/Projects/RLxEvolution/RLxEvolution/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1659: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=164525)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=164525)\u001b[0m If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rhys/Projects/RLxEvolution/RLxEvolution/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1659: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-03 16:17:47,323\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "SELECT_ENV = \"Taxi-v3\"\n",
    "N_ITER = 10\n",
    "\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config[\"log_level\"] = \"WARN\"\n",
    "\n",
    "agent = ppo.PPOTrainer(config, env=SELECT_ENV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-03 16:17:49,880\tWARNING deprecation.py:33 -- DeprecationWarning: `SampleBatch.data[..]` has been deprecated. Use `SampleBatch[..]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1: Min/Mean/Max reward: -884.0000/-798.0500/-686.0000, len mean: 200.0000. Checkpoint saved to tmp/ppo/taxi/checkpoint_000001/checkpoint-1\n",
      "  2: Min/Mean/Max reward: -884.0000/-769.2500/-650.0000, len mean: 200.0000. Checkpoint saved to tmp/ppo/taxi/checkpoint_000002/checkpoint-2\n",
      "  3: Min/Mean/Max reward: -884.0000/-744.5000/-542.0000, len mean: 200.0000. Checkpoint saved to tmp/ppo/taxi/checkpoint_000003/checkpoint-3\n",
      "  4: Min/Mean/Max reward: -884.0000/-728.8625/-542.0000, len mean: 200.0000. Checkpoint saved to tmp/ppo/taxi/checkpoint_000004/checkpoint-4\n",
      "  5: Min/Mean/Max reward: -884.0000/-712.5500/-515.0000, len mean: 200.0000. Checkpoint saved to tmp/ppo/taxi/checkpoint_000005/checkpoint-5\n",
      "  6: Min/Mean/Max reward: -848.0000/-675.1100/-434.0000, len mean: 200.0000. Checkpoint saved to tmp/ppo/taxi/checkpoint_000006/checkpoint-6\n",
      "  7: Min/Mean/Max reward: -848.0000/-643.2500/-425.0000, len mean: 200.0000. Checkpoint saved to tmp/ppo/taxi/checkpoint_000007/checkpoint-7\n",
      "  8: Min/Mean/Max reward: -821.0000/-614.5400/-425.0000, len mean: 200.0000. Checkpoint saved to tmp/ppo/taxi/checkpoint_000008/checkpoint-8\n",
      "  9: Min/Mean/Max reward: -821.0000/-582.6800/-425.0000, len mean: 200.0000. Checkpoint saved to tmp/ppo/taxi/checkpoint_000009/checkpoint-9\n",
      " 10: Min/Mean/Max reward: -821.0000/-547.1200/-280.0000, len mean: 199.3000. Checkpoint saved to tmp/ppo/taxi/checkpoint_000010/checkpoint-10\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "episode_data = []\n",
    "episode_json = []\n",
    "\n",
    "for n in range(N_ITER):\n",
    "    result = agent.train()\n",
    "    results.append(result)\n",
    "    \n",
    "    episode = {'n': n, \n",
    "               'episode_reward_min': result['episode_reward_min'], \n",
    "               'episode_reward_mean': result['episode_reward_mean'], \n",
    "               'episode_reward_max': result['episode_reward_max'],  \n",
    "               'episode_len_mean': result['episode_len_mean']\n",
    "              }\n",
    "    \n",
    "    episode_data.append(episode)\n",
    "    episode_json.append(json.dumps(episode))\n",
    "    file_name = agent.save(checkpoint_root)\n",
    "    \n",
    "    print(f'{n+1:3d}: Min/Mean/Max reward: {result[\"episode_reward_min\"]:8.4f}/{result[\"episode_reward_mean\"]:8.4f}/{result[\"episode_reward_max\"]:8.4f}, len mean: {result[\"episode_len_mean\"]:8.4f}. Checkpoint saved to {file_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gp1LgeCJjGLk"
   },
   "source": [
    "Do the episode rewards increase after multiple iterations?\n",
    "\n",
    "Also, print out the policy and model to see the results of training in detail…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'default_policy/fc_1/kernel:0' shape=(500, 256) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_1/bias:0' shape=(256,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_1/kernel:0' shape=(500, 256) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_1/bias:0' shape=(256,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_2/kernel:0' shape=(256, 256) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_2/bias:0' shape=(256,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_2/kernel:0' shape=(256, 256) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_value_2/bias:0' shape=(256,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_out/kernel:0' shape=(256, 6) dtype=float32>,\n",
      " <tf.Variable 'default_policy/fc_out/bias:0' shape=(6,) dtype=float32>,\n",
      " <tf.Variable 'default_policy/value_out/kernel:0' shape=(256, 1) dtype=float32>,\n",
      " <tf.Variable 'default_policy/value_out/bias:0' shape=(1,) dtype=float32>]\n",
      "<tf.Tensor 'Reshape:0' shape=(?,) dtype=float32>\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "observations (InputLayer)       [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "fc_1 (Dense)                    (None, 256)          128256      observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_1 (Dense)              (None, 256)          128256      observations[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_2 (Dense)                    (None, 256)          65792       fc_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "fc_value_2 (Dense)              (None, 256)          65792       fc_value_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "fc_out (Dense)                  (None, 6)            1542        fc_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "value_out (Dense)               (None, 1)            257         fc_value_2[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 389,895\n",
      "Trainable params: 389,895\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "policy = agent.get_policy()\n",
    "model = policy.model\n",
    "\n",
    "pprint.pprint(model.variables())\n",
    "pprint.pprint(model.value_function())\n",
    "\n",
    "print(model.base_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollout\n",
    "\n",
    "Next we'll use the [`rollout` script](https://ray.readthedocs.io/en/latest/rllib-training.html#evaluating-trained-policies) to evaluate the trained policy.\n",
    "\n",
    "The output from the following command visualizes the \"taxi\" agent operating within its simulation: picking up a passenger, driving, turning, dropping off a passenger (\"put-down\"), and so on. \n",
    "\n",
    "A 2-D map of the *observation space* is visualized as text, which needs some decoding instructions:\n",
    "\n",
    "  * `R` -- R(ed) location in the Northwest corner\n",
    "  * `G` -- G(reen) location in the Northeast corner\n",
    "  * `Y` -- Y(ellow) location in the Southwest corner\n",
    "  * `B` -- B(lue) location in the Southeast corner\n",
    "  * `:` -- cells where the taxi can drive\n",
    "  * `|` -- obstructions (\"walls\") which the taxi must avoid\n",
    "  * blue letter represents the current passenger’s location for pick-up\n",
    "  * purple letter represents the drop-off location\n",
    "  * yellow rectangle is the current location of our taxi/agent\n",
    "\n",
    "That allows for a total of 500 states, and these known states are numbered between 0 and 499.\n",
    "\n",
    "The *action space* for the taxi/agent is defined as:\n",
    "\n",
    "  * move the taxi one square North\n",
    "  * move the taxi one square South\n",
    "  * move the taxi one square East\n",
    "  * move the taxi one square West\n",
    "  * pick-up the passenger\n",
    "  * put-down the passenger\n",
    "\n",
    "The *rewards* are structured as −1 for each action plus:\n",
    "\n",
    " * +20 points when the taxi performs a correct drop-off for the passenger\n",
    " * -10 points when the taxi attempts illegal pick-up/drop-off actions\n",
    "\n",
    "Admittedly it'd be better if these state visualizations showed the *reward* along with observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rhys/Projects/RLxEvolution/RLxEvolution/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rhys/Projects/RLxEvolution/RLxEvolution/bin/rllib\", line 8, in <module>\n",
      "    sys.exit(cli())\n",
      "  File \"/home/rhys/Projects/RLxEvolution/RLxEvolution/lib/python3.8/site-packages/ray/rllib/scripts.py\", line 36, in cli\n",
      "    rollout.run(options, rollout_parser)\n",
      "  File \"/home/rhys/Projects/RLxEvolution/RLxEvolution/lib/python3.8/site-packages/ray/rllib/rollout.py\", line 277, in run\n",
      "    raise ValueError(\n",
      "ValueError: Could not find params.pkl in either the checkpoint dir or its parent directory AND no `--config` given on command line!\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!rllib rollout \\\n",
    "    tmp/ppo/taxi/checkpoint_000010/checkpoint-10 \\\n",
    "    --config \"{\\\"env\\\": \\\"Taxi-v3\\\"}\" \\\n",
    "    --run PPO \\\n",
    "    --steps 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()  # \"Undo ray.init()\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise (\"Homework\")\n",
    "\n",
    "In addition to _Taxi_, there are other so-called [\"toy text\"](https://gym.openai.com/envs/#toy_text) problems you can try."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of rllib_ppo_dqn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "RLxEvolution",
   "language": "python",
   "name": "rlxevolution"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
